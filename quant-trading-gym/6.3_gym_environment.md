# V6.3: Gym Environment

## Overview

New crate `crates/gym/` providing a gymnasium-compatible `TradingEnv` that wraps a full `Simulation` (with background agents) and exposes a single RL agent slot via step/reset/seed API. This is the bridge to V7 reinforcement learning.

**Requires**: V6.1 complete (`FullFeatures` extractor available), V6.2 recommended (ensemble provides baseline agent)

## Design Decisions

### 1. One Simulation Per Episode

`TradingEnv::reset()` creates a **fresh `Simulation`** rather than resetting an existing one. Rationale:
- Avoids complex subsystem reset logic (14 phases, 6 subsystems)
- `Simulation::new()` is fast (no heavy initialization)
- Guarantees clean state — no subtle bugs from incomplete resets
- Seed support is natural: pass seed to `SimulationConfig`

### 2. ExternalAgent Pattern

The RL agent is a standard `Agent` impl whose action is set externally via `Arc<Mutex<>>`:

```rust
pub struct ExternalAgent {
    id: AgentId,
    state: AgentState,
    pending_action: Arc<Mutex<DiscreteAction>>,
    config: ExternalAgentConfig,
}

impl Agent for ExternalAgent {
    fn on_tick(&mut self, ctx: &StrategyContext<'_>) -> AgentAction {
        let action = *self.pending_action.lock().unwrap();
        match action {
            DiscreteAction::Sell => self.generate_sell_order(ctx),
            DiscreteAction::Hold => AgentAction::none(),
            DiscreteAction::Buy => self.generate_buy_order(ctx),
        }
    }

    fn on_fill(&mut self, trade: &Trade) {
        // Standard position tracking (same as TreeAgent)
        if trade.buyer_id == self.id { self.state.on_buy(...); }
        if trade.seller_id == self.id { self.state.on_sell(...); }
    }

    fn state(&self) -> &AgentState { &self.state }
}
```

**Why `Arc<Mutex<>>`**: The gym sets the action before calling `sim.step()`, which calls `on_tick()` during Phase 4. Since this is single-threaded (set → step → set → step), the mutex never contends. But `Arc<Mutex<>>` is required because `Agent: Send` and the `Arc` is shared between `TradingEnv` and the agent inside `Simulation`.

### 3. Discrete Action Space

```rust
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum DiscreteAction {
    Sell = 0,
    Hold = 1,
    Buy = 2,
}
```

Matches V5's 3-class classification `[p_sell, p_hold, p_buy]`. Fixed order size from config.

**V7 extension**: Add `ContinuousAction(f64)` for position sizing. Deferred — discrete is sufficient for V6 ensemble baseline.

### 4. Pluggable Reward Function

```rust
pub trait RewardFunction: Send + Sync {
    fn compute(
        &self,
        prev: &EpisodeState,
        action: DiscreteAction,
        curr: &EpisodeState,
    ) -> f64;
}
```

**Default implementation** — P&L delta:
```rust
pub struct PnlDeltaReward;

impl RewardFunction for PnlDeltaReward {
    fn compute(&self, prev: &EpisodeState, _action: DiscreteAction, curr: &EpisodeState) -> f64 {
        (curr.equity - prev.equity) // Simple equity change
    }
}
```

**V7.1 implementation** (prepared, not used in V6):
```rust
pub struct RiskAdjustedReward {
    pub drawdown_penalty: f64,
    pub transaction_cost_per_trade: f64,
    pub sharpe_terminal_bonus: f64,
}

impl RewardFunction for RiskAdjustedReward {
    fn compute(&self, prev: &EpisodeState, action: DiscreteAction, curr: &EpisodeState) -> f64 {
        let pnl = curr.equity - prev.equity;
        let drawdown = (curr.peak_equity - curr.equity).max(0.0) * self.drawdown_penalty;
        let tx_cost = if action != DiscreteAction::Hold { self.transaction_cost_per_trade } else { 0.0 };
        pnl - drawdown - tx_cost
        // Terminal Sharpe bonus added in is_terminal() check
    }
}
```

### 5. Episode Management

```rust
pub struct EpisodeState {
    pub tick: u64,
    pub cash: f64,
    pub equity: f64,
    pub realized_pnl: f64,
    pub peak_equity: f64,
    pub mid_price: f64,
}

pub struct TradingEnvConfig {
    pub sim_config: SimulationConfig,
    pub agent_config: ExternalAgentConfig,
    pub max_ticks: u64,          // Episode length (default: 10_000)
    pub seed: Option<u64>,       // None = random each reset
    pub reward_fn: Box<dyn RewardFunction>,
    pub warmup_ticks: u64,       // Ticks before RL agent starts acting (default: 100)
}
```

**Termination conditions**:
- `tick_count >= max_ticks` → terminated (natural end)
- `equity <= 0` → terminated (bankrupt)
- No truncation in V6 (V7 may add time-limit truncation)

**Warmup**: The RL agent holds for the first `warmup_ticks` to allow indicators and background agents to initialize. Observations are still returned during warmup.

---

## Core Type: TradingEnv

```rust
pub struct TradingEnv {
    sim: Simulation,
    agent_id: AgentId,
    action_handle: Arc<Mutex<DiscreteAction>>,
    extractor: FullFeatures,
    reward_fn: Box<dyn RewardFunction>,
    prev_state: EpisodeState,
    config: TradingEnvConfig,
    tick_count: u64,
}

impl TradingEnv {
    pub fn new(config: TradingEnvConfig) -> Self {
        let (sim, agent_id, action_handle) = Self::build_simulation(&config);
        let prev_state = EpisodeState::initial(&config);
        Self { sim, agent_id, action_handle, extractor: FullFeatures, reward_fn: config.reward_fn, prev_state, config, tick_count: 0 }
    }

    pub fn reset(&mut self, seed: Option<u64>) -> Vec<f64> {
        let actual_seed = seed.or(self.config.seed);
        let mut config = self.config.sim_config.clone();
        if let Some(s) = actual_seed { config.seed = s; }

        let (sim, agent_id, action_handle) = Self::build_simulation_with_config(&config, &self.config);
        self.sim = sim;
        self.agent_id = agent_id;
        self.action_handle = action_handle;
        self.tick_count = 0;
        self.prev_state = EpisodeState::initial(&self.config);

        // Run warmup ticks (agent holds)
        for _ in 0..self.config.warmup_ticks {
            self.sim.step();
            self.tick_count += 1;
        }

        self.extract_observation()
    }

    pub fn step(&mut self, action: DiscreteAction) -> StepResult {
        // 1. Set pending action
        *self.action_handle.lock().unwrap() = action;

        // 2. Advance simulation one tick
        self.sim.step();
        self.tick_count += 1;

        // 3. Extract observation
        let obs = self.extract_observation();

        // 4. Compute reward
        let curr_state = self.capture_episode_state();
        let reward = self.reward_fn.compute(&self.prev_state, action, &curr_state);
        self.prev_state = curr_state;

        // 5. Check termination
        let terminated = self.tick_count >= self.config.max_ticks
            || self.prev_state.equity <= 0.0;

        StepResult {
            observation: obs,
            reward,
            terminated,
            truncated: false,
            info: StepInfo {
                tick: self.tick_count,
                equity: self.prev_state.equity,
                realized_pnl: self.prev_state.realized_pnl,
            },
        }
    }

    pub fn observation_size(&self) -> usize { self.extractor.n_features() }
    pub fn action_size(&self) -> usize { 3 }

    fn build_simulation(config: &TradingEnvConfig) -> (Simulation, AgentId, Arc<Mutex<DiscreteAction>>) {
        let mut sim = Simulation::new(config.sim_config.clone());

        // Spawn background agents via AgentFactory
        let factory = AgentFactory::new(config.sim_config.clone());
        factory.spawn_all(&mut sim);

        // Add the RL external agent
        let action_handle = Arc::new(Mutex::new(DiscreteAction::Hold));
        let rl_agent_id = AgentId(99_999); // Reserved ID
        let agent = ExternalAgent::new(rl_agent_id, action_handle.clone(), config.agent_config.clone());
        sim.add_agent(Box::new(agent));

        (sim, rl_agent_id, action_handle)
    }

    fn extract_observation(&self) -> Vec<f64> {
        // Use the primary symbol for now (single-symbol gym)
        let symbol = self.sim.config().symbols().first().cloned()
            .unwrap_or_else(|| "ACME".to_string());

        // Extract features using FullFeatures extractor
        // This requires building a temporary StrategyContext or reading from the ML cache
        // Implementation depends on what Simulation exposes (agent_state, market_view, etc.)
        todo!("Extract observation from simulation state")
    }

    fn capture_episode_state(&self) -> EpisodeState {
        let agent_state = self.sim.agent_state(self.agent_id).expect("RL agent exists");
        let symbol = self.sim.config().symbols().first().cloned().unwrap();
        let mid_price = self.sim.market().mid_price(&symbol)
            .map(|p| p.to_float()).unwrap_or(100.0);
        let equity = agent_state.cash().to_float()
            + agent_state.position_for(&symbol) as f64 * mid_price;

        EpisodeState {
            tick: self.tick_count,
            cash: agent_state.cash().to_float(),
            equity,
            realized_pnl: agent_state.realized_pnl().to_float(),
            peak_equity: self.prev_state.peak_equity.max(equity),
            mid_price,
        }
    }
}
```

---

## Crate Structure

```
crates/gym/
├── Cargo.toml
│   [dependencies]
│   simulation = { path = "../simulation" }
│   agents = { path = "../agents" }
│   types = { path = "../types" }
│   news = { path = "../news" }
│
├── src/
│   ├── lib.rs              # pub mod trading_env, external_agent, reward, episode
│   ├── trading_env.rs      # TradingEnv (step/reset/seed)
│   ├── external_agent.rs   # ExternalAgent + DiscreteAction
│   ├── reward.rs           # RewardFunction trait + PnlDeltaReward + RiskAdjustedReward
│   └── episode.rs          # EpisodeState, StepResult, StepInfo, termination logic
```

**Workspace**: Add `"crates/gym"` to `Cargo.toml` workspace members.

---

## Integration Points

- **AgentFactory** (from refactor): `TradingEnv::build_simulation()` calls `factory.spawn_all()` to populate background agents
- **Simulation::agent_state()** (from refactor): Gym reads RL agent's position/cash/P&L after each step
- **FullFeatures** (from V6.1): `extract_observation()` uses the same feature extractor as ensemble training
- **SimulationConfig.seed**: Passed through for deterministic episode generation

---

## Open Question: Observation Extraction

The gym needs to extract the 54-feature observation vector after `sim.step()`. Two approaches:

**Option A**: Read from the ML prediction cache (if `ModelRegistry` is configured with `FullFeatures`). Cache is populated in Phase 3, before agents act. After `step()`, the cache still contains this tick's features.

**Option B**: Re-extract features from simulation state. Requires building a temporary `StrategyContext` from the Simulation's internal state (market, candles, indicators, events). Needs `Simulation::observation_context()` API.

**Recommendation**: Option A is simpler if the ML cache is available. Option B is a fallback. Implement both and use whichever is available.

---

## Verification

1. `cargo test -p gym` — unit tests for ExternalAgent, reward computation, episode termination
2. Integration test: `TradingEnv::new() → reset() → 100x step(Hold)` — no panics, observations valid
3. Integration test: `reset(seed=42) → run episode → reset(seed=42) → run episode` — identical observations (deterministic)
4. Integration test: random actions for 10k steps — agent trades, equity changes, episode terminates at max_ticks
5. Reward test: known P&L scenario → verify PnlDeltaReward produces correct values
