# V6.1: Full Feature Engineering

## Overview

Expand the V5 42-feature market schema to 55 centralized market features (42 base + 13 new). Each new feature is chosen for its **predictive signal** in trading decisions, not added mechanically. All features are market-level (shared across agents, cacheable).

**Requires**: Pre-V6 refactor complete (sections A-F: `FeatureExtractor` trait, `FeatureVec` cache, agent factory, gym API, schema-flexible recording)

**Note**: Base 42 features are kept stable (no trimming in V6.1). Trimming deferred to post-V6.2 after SHAP analysis provides empirical evidence. This preserves V5 model compatibility and V6.2 mixed-model support.

## Feature Design Principles

- **Signal over count**: More features is not better. Every feature must justify its existence with predictive value. Trim what doesn't work.
- **Distinct signals**: Each feature captures a different market regime (momentum, mean-reversion, liquidity, volatility, value). No redundant features.
- **Normalized scales**: Use ratios, basis points, and z-scores instead of raw prices. Trees don't need normalization, but V7.2 deep RL will — build the habit now.
- **No lookahead**: Only data available at decision time. Features computed from candles, indicators, and book state that exist before the agent acts.
- **No sunk-cost features**: No per-agent portfolio state (position, unrealized P&L). These encode behavioral biases (disposition effect, loss aversion) rather than predictive market signal. Position limits are enforced by `PositionValidator`. Risk management belongs in reward shaping (V7.1), not feature engineering.
- **Grouped by signal type**: Enables ablation testing (disable one group, measure impact).

---

## Audit of Existing 42 Features

Before adding features, evaluate which of the existing 42 carry signal and which are candidates for removal.

### Likely High-Value (keep)

| Features | Count | Rationale |
|----------|-------|-----------|
| `f_price_change_{1,2,3,4}` | 4 | Short-term momentum. Most predictive lookbacks for batch auction frequency. |
| `f_log_return_{1,2,3,4}` | 4 | Same signal as price change but better for multiplicative models. Keep both — trees will use whichever splits better. |
| `f_rsi_8` | 1 | Mean-reversion signal. Well-established. |
| `f_macd_line`, `f_macd_histogram` | 2 | Trend direction and acceleration. Core momentum signals. |
| `f_bb_percent_b` | 1 | Normalized position within volatility bands. Good mean-reversion trigger. |
| `f_atr_8` | 1 | Volatility measure. Essential for normalizing other features. |
| `f_has_active_news`, `f_news_sentiment` | 2 | Event-driven signal. Direct catalyst for moves. |

### Possibly Redundant (validate empirically)

| Features | Count | Concern |
|----------|-------|---------|
| `f_price_change_{6,8,12,16,24,32,48,64}` | 8 | Longer lookbacks may be noise at batch-auction frequency. Keep if MI > 0.01. |
| `f_log_return_{6,8,...,64}` | 8 | Same concern. Highly correlated with corresponding price_change. May be redundant. |
| `f_sma_8`, `f_sma_16`, `f_ema_8`, `f_ema_16` | 4 | Raw indicator values are price-dependent. `f_trend_strength` (EMA gap / ATR) may be strictly better. |
| `f_bb_upper`, `f_bb_middle`, `f_bb_lower` | 3 | Raw Bollinger values are price-dependent. `f_bb_percent_b` already captures the normalized position. These 3 may be redundant. |
| `f_macd_signal` | 1 | Highly correlated with `f_macd_line`. Histogram = line - signal, so signal is derivable. |
| `f_mid_price` | 1 | Raw price level. Not a signal — AAPL at $150 vs $200 doesn't predict direction. Only useful as denominator. |
| `f_news_magnitude`, `f_news_ticks_remaining` | 2 | May not carry incremental signal over sentiment. Validate. |

### Recommended Validation Process

1. Record 50k ticks with all 42 features
2. Compute mutual information of each feature vs next-tick return sign
3. Compute pairwise Pearson correlation
4. Remove candidates: MI < 0.005 AND no unique information (covered by correlated feature)
5. Retrain V5 tree model with reduced set, compare accuracy

**Expected outcome**: Some base features will have low MI / high redundancy. However, **trimming is deferred to post-V6.2** to preserve V5 model index compatibility. Dead-weight features are kept as stable indices — trees ignore them. Trimming happens after SHAP analysis provides empirical evidence.

### Additional Feature Candidates (Round 2, if validation shows gaps)

These are not in the initial 13 but could be added if empirical analysis shows the initial set misses important signals:

| Feature | Formula | Signal | When to Add |
|---------|---------|--------|-------------|
| `f_return_autocorrelation` | `corr(returns[-8:], returns[-16:-8])` | Trending (+) vs mean-reverting (-) regime | If vol features alone don't capture regime |
| `f_momentum_persistence` | Count of consecutive same-sign returns | Trend durability/exhaustion | If trend_strength lacks predictive power |
| `f_vwap_deviation` | `(mid - vwap) / vwap` | Price vs volume-weighted average | Requires VWAP tracking infrastructure |
| `f_avg_trade_size` | `sum(trade_volumes) / n_trades` | Informed vs noise flow | If trade_intensity alone is insufficient |
| `f_cross_symbol_corr` | Rolling correlation between symbols | Market-wide risk regime | Only for multi-symbol configurations |
| `f_price_acceleration` | `price_change_1 - price_change_2` | Second derivative of price | If momentum features miss turning points |

**Process**: Start with the 13 new features. After V6.2 ensemble training, run SHAP analysis. If feature importance is concentrated in <5 features, the rest may be noise. Add Round 2 candidates to fill gaps, re-validate.

---

## New Centralized Market Features (13 new, indices 42-54)

### Microstructure Signals (liquidity & flow)

| Index | Name | Formula | Signal | Why It Matters |
|-------|------|---------|--------|----------------|
| 42 | `f_spread_bps` | `(ask - bid) / mid * 10000` | Liquidity cost | Wide spread = illiquid, risky to trade. Tight = safe. Reflects market maker confidence. |
| 43 | `f_book_imbalance` | `(bid_vol - ask_vol) / (bid_vol + ask_vol)` | Residual order flow | Batch auction clears books each tick. Remaining orders reflect the *previous* tick's unmatched flow — what agents collectively submitted. Positive = net buying pressure last tick. Acts as crowd sentiment indicator. |
| 54 | `f_net_order_flow` | `(n_buyers - n_sellers) / (n_buyers + n_sellers)` | Agent count imbalance | Measures directional participation from last tick's executed trades. 10 buyers and 2 sellers = broad buying interest through few liquidity providers. Normalized to [-1, 1]. |

**Caveat on book_imbalance**: In batch auctions, book imbalance measures *residual* order flow, not persistent supply/demand. May be noise — evaluate empirically alongside `f_net_order_flow` in post-V6.2 SHAP analysis.

**Caveat on net_order_flow**: In batch auctions, every trade has equal buy and sell **volume** (no aggressor side). So `(buy_vol - sell_vol) / total = 0` always. The formula uses **unique agent counts** instead: `n_buyers = |{trade.buyer_id}|`, `n_sellers = |{trade.seller_id}|` from `ctx.recent_trades(symbol)`. Division by zero if no trades → NaN.

**Data sources**:
- `f_spread_bps`: `ctx.best_bid()`, `ctx.best_ask()`, `ctx.mid_price()`
- `f_book_imbalance`: `ctx.total_bid_volume()`, `ctx.total_ask_volume()`
- `f_net_order_flow`: `ctx.recent_trades(symbol)` → unique `buyer_id` and `seller_id` counts

### Volatility Regime (risk & opportunity sizing)

| Index | Name | Formula | Signal | Why It Matters |
|-------|------|---------|--------|----------------|
| 44 | `f_realized_vol_8` | `std(log_returns[-8:])` | Short-term volatility | High vol = larger expected moves. Models should size positions inversely to vol. |
| 45 | `f_realized_vol_32` | `std(log_returns[-32:])` | Long-term vol baseline | Stable reference. Low vol for extended periods often precedes breakouts. |
| 46 | `f_vol_ratio` | `vol_8 / vol_32` | Regime change indicator | >1 = vol expanding (breakout likely), <1 = vol contracting (mean-reversion regime). |

**Data source**: Computed from sequential 1-period log returns over candle history. Requires candle history of 32+ ticks (available after warmup).

**Pure computation** (added to `types/src/features.rs`):
```rust
/// Compute realized volatility (standard deviation of sequential 1-period log returns).
///
/// IMPORTANT: Uses sequential returns ln(close[t] / close[t-1]) for consecutive candles,
/// NOT cumulative returns from `log_return_from_candles()` which computes
/// ln(close[-1] / close[-(i+1)]) — overlapping cumulative returns whose std dev
/// mechanically increases with horizon and is NOT realized volatility.
pub fn realized_volatility(candles: &[Candle], lookback: usize) -> f64 {
    if candles.len() < lookback + 1 { return f64::NAN; }
    let n = candles.len();
    let returns: Vec<f64> = (0..lookback)
        .map(|i| {
            let current = candles[n - 1 - i].close.to_float();
            let previous = candles[n - 2 - i].close.to_float();
            if current > 0.0 && previous > 0.0 {
                (current / previous).ln()
            } else {
                f64::NAN
            }
        })
        .filter(|r| r.is_finite())
        .collect();
    if returns.len() < 2 { return f64::NAN; }
    let mean = returns.iter().sum::<f64>() / returns.len() as f64;
    let variance = returns.iter().map(|r| (r - mean).powi(2)).sum::<f64>() / (returns.len() - 1) as f64;
    variance.sqrt()
}
```

### Fundamental Value (mean-reversion anchor)

| Index | Name | Formula | Signal | Why It Matters |
|-------|------|---------|--------|----------------|
| 47 | `f_fair_value_dev` | `(mid - fair_value) / fair_value` | Mispricing magnitude | Deviation from Gordon Growth Model fair value. Large positive = overpriced, negative = underpriced. Mean-reversion signal. |
| 48 | `f_price_to_fair` | `mid / fair_value` | Valuation ratio | >1.0 overvalued, <1.0 undervalued. Simple, interpretable. Trees can split on this directly. |

**Data source**: `ctx.fair_value(symbol)` (V2.4 fundamentals system). Returns `None` if no fundamentals configured — impute with NaN → per-feature neutral value (0.0 for deviation, 1.0 for ratio).

### Momentum Quality (trend strength)

| Index | Name | Formula | Signal | Why It Matters |
|-------|------|---------|--------|----------------|
| 49 | `f_trend_strength` | `abs(ema_8 - ema_16) / atr_8` | Normalized trend magnitude | Raw EMA gap is price-dependent. Dividing by ATR normalizes across price levels and volatility regimes. Values >2 indicate strong trends. |
| 50 | `f_rsi_divergence` | `rsi_8 - 50.0` | Directional conviction | RSI centered at 0. Range [-50, +50]. Avoids the overbought/oversold boundaries (30/70) which are regime-dependent. Trees learn their own optimal thresholds. |

**Data source**: `ctx.get_indicator(symbol, IndicatorType::Ema(8))`, `ctx.get_indicator(symbol, IndicatorType::Rsi(8))`, etc. — all pre-computed in V5 `IndicatorSnapshot`.

### Volume Dynamics & Cross-Feature

| Index | Name | Formula | Signal | Why It Matters |
|-------|------|---------|--------|----------------|
| 51 | `f_volume_surge` | `latest_candle.volume / avg_volume_8` | Volume spike detection | Abnormal volume often precedes or confirms directional moves. Ratio normalizes across symbols. |
| 52 | `f_trade_intensity` | `n_recent_trades / 8.0` | Activity level | Trade frequency proxy. High intensity = market attention. Low = quiet, less predictable. |
| 53 | `f_sentiment_price_gap` | `symbol_sentiment * fair_value_dev` | Interaction term | Positive when news sentiment aligns with fundamental mispricing. Negative when they conflict. Captures "smart money" signal — news + value agreement is stronger than either alone. |

**`f_volume_surge` computation detail**: No pre-computed volume SMA exists in the indicator system. Compute inline from candle history. The average EXCLUDES the latest candle to avoid dampening the signal (a 10x spike should show as ~10x, not ~2.3x):
```rust
let candles = ctx.candles(symbol);
let n_prev = candles.len().saturating_sub(1).min(8);
let vol_avg = if n_prev > 0 {
    candles.iter().rev().skip(1).take(8).map(|c| c.volume.raw() as f64).sum::<f64>() / n_prev as f64
} else { 0.0 };
let latest_vol = ctx.last_candle(symbol).map(|c| c.volume.raw() as f64).unwrap_or(f64::NAN);
// NaN if no previous candles or zero average volume (early warmup)
if vol_avg > 0.0 { latest_vol / vol_avg } else { f64::NAN }
```

**`f_trade_intensity` window**: `ctx.recent_trades(symbol)` returns trades from the **previous tick** (updated in Phase 10, after the auction; StrategyContext is built in Phase 3, before the auction). The normalizer `TRADE_INTENSITY_BASELINE = 8.0` represents a baseline trade count per tick.

**`f_sentiment_price_gap` NaN handling**: This interaction term MUST be computed **before** the NaN imputation pass. If `f_fair_value_dev` is NaN (no fundamentals configured), the product must also be NaN — not `sentiment * -1.0` which produces a spurious signal:
```rust
let sentiment = ctx.symbol_sentiment(symbol);
let fv_dev = features[extended_idx::FAIR_VALUE_DEV]; // Still raw NaN at this point
features[extended_idx::SENTIMENT_PRICE_GAP] = if fv_dev.is_nan() {
    f64::NAN  // Correctly marks as missing, imputed to neutral later
} else {
    sentiment * fv_dev
};
```

**Data sources**: Candle volume from `ctx.last_candle()`, trade count from `ctx.recent_trades()`, sentiment from `ctx.symbol_sentiment()`.

---

## Why No Per-Agent Portfolio Features

Per-agent features (position, cash, unrealized P&L) were considered and rejected:

- **Sunk cost fallacy**: Current position is sunk cost. The optimal action depends on where the market is going, not where the agent entered. Encoding position biases toward disposition effect (hold losers, sell winners).
- **Position limits already enforced**: `PositionValidator` rejects orders that exceed limits. The model doesn't need to "know" its position to avoid overextension — the system handles it.
- **Risk management belongs in rewards**: V7.1 reward shaping (drawdown penalty, volatility penalty) is the correct mechanism for position risk, not feature engineering.
- **Simplifies architecture**: All features are centralized and cacheable. EnsembleAgent uses cached features directly, same as TreeAgent. No two-tier feature model needed.

If V7 RL shows evidence that position-awareness improves performance, portfolio features can be added as an experiment. But they should not be assumed useful.

---

## Implementation

### New Constants (`crates/types/src/features.rs`)

```rust
pub const N_FULL_FEATURES: usize = 55; // 42 base + 13 new

pub mod extended_idx {
    pub const SPREAD_BPS: usize = 42;
    pub const BOOK_IMBALANCE: usize = 43;
    pub const REALIZED_VOL_8: usize = 44;
    pub const REALIZED_VOL_32: usize = 45;
    pub const VOL_RATIO: usize = 46;
    pub const FAIR_VALUE_DEV: usize = 47;
    pub const PRICE_TO_FAIR: usize = 48;
    pub const TREND_STRENGTH: usize = 49;
    pub const RSI_DIVERGENCE: usize = 50;
    pub const VOLUME_SURGE: usize = 51;
    pub const TRADE_INTENSITY: usize = 52;
    pub const SENTIMENT_PRICE_GAP: usize = 53;
    pub const NET_ORDER_FLOW: usize = 54;
}

pub const FULL_FEATURE_NAMES: &[&str] = &[
    // First 42 from MARKET_FEATURE_NAMES...
    // Then 13 new:
    "f_spread_bps", "f_book_imbalance",
    "f_realized_vol_8", "f_realized_vol_32", "f_vol_ratio",
    "f_fair_value_dev", "f_price_to_fair",
    "f_trend_strength", "f_rsi_divergence",
    "f_volume_surge", "f_trade_intensity", "f_sentiment_price_gap",
    "f_net_order_flow",
];
```

Compile-time assertion (Rust stable since 1.79):
```rust
const _: () = assert!(FULL_FEATURE_NAMES.len() == N_FULL_FEATURES);
```

### Per-Feature Neutral Values for NaN Imputation (`crates/types/src/features.rs`)

`FullFeatures` uses **per-feature neutral values** instead of uniform -1.0. `MinimalFeatures` keeps -1.0 for V5 backward compat.

```rust
/// Neutral imputation values for FullFeatures.
/// Each value represents "no signal" for that feature type.
/// Exported to Python training scripts to replace np.nan_to_num(X, nan=-1.0).
pub const FULL_FEATURE_NEUTRAL: [f64; N_FULL_FEATURES] = {
    let mut v = [-1.0f64; N_FULL_FEATURES];

    // Price changes & log returns → 0.0 (no change = neutral)
    let mut i = idx::PRICE_CHANGE_START;
    while i < idx::PRICE_CHANGE_START + N_LOOKBACKS { v[i] = 0.0; i += 1; }
    i = idx::LOG_RETURN_START;
    while i < idx::LOG_RETURN_START + N_LOOKBACKS { v[i] = 0.0; i += 1; }

    // Technical indicators
    // SMA, EMA, BB raw values: keep -1.0 (raw price scale, not meaningful for NN)
    v[idx::RSI_8] = 50.0;          // RSI neutral
    v[idx::MACD_LINE] = 0.0;       // No trend
    v[idx::MACD_SIGNAL] = 0.0;
    v[idx::MACD_HISTOGRAM] = 0.0;
    v[idx::BB_PERCENT_B] = 0.5;    // Middle of band
    // ATR: keep -1.0 (absolute scale)

    // News → 0.0 (no news = no signal)
    v[idx::HAS_ACTIVE_NEWS] = 0.0;
    v[idx::NEWS_SENTIMENT] = 0.0;
    v[idx::NEWS_MAGNITUDE] = 0.0;
    v[idx::NEWS_TICKS_REMAINING] = 0.0;

    // New features
    v[extended_idx::SPREAD_BPS] = 0.0;          // Tight spread assumed
    v[extended_idx::BOOK_IMBALANCE] = 0.0;      // Balanced
    v[extended_idx::REALIZED_VOL_8] = 0.0;      // Zero vol during warmup
    v[extended_idx::REALIZED_VOL_32] = 0.0;
    v[extended_idx::VOL_RATIO] = 1.0;           // No regime change
    v[extended_idx::FAIR_VALUE_DEV] = 0.0;      // At fair value
    v[extended_idx::PRICE_TO_FAIR] = 1.0;       // At fair value
    v[extended_idx::TREND_STRENGTH] = 0.0;      // No trend
    v[extended_idx::RSI_DIVERGENCE] = 0.0;      // Neutral RSI
    v[extended_idx::VOLUME_SURGE] = 1.0;        // Normal volume
    v[extended_idx::TRADE_INTENSITY] = 0.0;     // No trades
    v[extended_idx::SENTIMENT_PRICE_GAP] = 0.0; // No signal
    v[extended_idx::NET_ORDER_FLOW] = 0.0;      // Balanced

    v
};
```

**Why per-feature neutrals instead of uniform -1.0**:
- **Trees**: Split thresholds are learned from data. Whether missing = -1.0 or 0.0, performance is identical.
- **Neural nets (V7.2)**: -1.0 falls within the normal range of many features (returns, sentiment). NN can't distinguish "missing" from "value is -1.0". Per-feature neutrals are semantically correct "no signal."
- **Training parity**: Export `FULL_FEATURE_NEUTRAL` to Python as the imputation map.

### FeatureExtractor Trait Extension

Add `neutral_values()` to the trait:
```rust
pub trait FeatureExtractor: Send + Sync {
    fn n_features(&self) -> usize;
    fn extract_market(&self, symbol: &Symbol, ctx: &StrategyContext<'_>) -> FeatureVec;
    fn feature_names(&self) -> &[&str];
    /// Per-feature neutral values for NaN imputation.
    fn neutral_values(&self) -> &[f64];
}
```
`MinimalFeatures::neutral_values()` returns `&[-1.0; 42]`. `FullFeatures::neutral_values()` returns `&FULL_FEATURE_NEUTRAL`.

**Note**: Base 42 features are NOT trimmed in V6.1. Trimming is deferred to post-V6.2 after SHAP analysis. V5 models trained on 42 features remain compatible (`model.predict(&features[..42])` works because V5 features are a prefix).

### FullFeatures Extractor (`crates/agents/src/tier1/ml/full_features.rs`)

```rust
pub struct FullFeatures;

impl FeatureExtractor for FullFeatures {
    fn n_features(&self) -> usize { N_FULL_FEATURES }

    fn extract_market(&self, symbol: &Symbol, ctx: &StrategyContext<'_>) -> FeatureVec {
        let mut features = smallvec::smallvec![f64::NAN; N_FULL_FEATURES];

        // Copy base 42 features from existing extractor (includes its own NaN→-1.0 pass)
        let base = extract_features(symbol, ctx);
        features[..N_MARKET_FEATURES].copy_from_slice(&base);
        // Reset base features back to NaN where -1.0 was imputed, so per-feature
        // neutral imputation handles them correctly at the end
        features[..N_MARKET_FEATURES].iter_mut().for_each(|f| if *f == -1.0 { *f = f64::NAN; });

        // Microstructure (42-43, 54)
        let mid = ctx.mid_price(symbol).map(|p| p.to_float()).unwrap_or(f64::NAN);
        let spread = ctx.spread(symbol).map(|p| p.to_float()).unwrap_or(f64::NAN);
        features[extended_idx::SPREAD_BPS] = if mid > 0.0 { spread / mid * 10000.0 } else { f64::NAN };

        let bid_vol = ctx.total_bid_volume(symbol).raw() as f64;
        let ask_vol = ctx.total_ask_volume(symbol).raw() as f64;
        let total_vol = bid_vol + ask_vol;
        features[extended_idx::BOOK_IMBALANCE] = if total_vol > 0.0 {
            (bid_vol - ask_vol) / total_vol
        } else { f64::NAN };

        // Net order flow (unique buyer/seller count ratio)
        let trades = ctx.recent_trades(symbol);
        if !trades.is_empty() {
            use std::collections::HashSet;
            let buyers: HashSet<_> = trades.iter().map(|t| t.buyer_id).collect();
            let sellers: HashSet<_> = trades.iter().map(|t| t.seller_id).collect();
            let n_b = buyers.len() as f64;
            let n_s = sellers.len() as f64;
            features[extended_idx::NET_ORDER_FLOW] = (n_b - n_s) / (n_b + n_s);
        }
        // else: remains NaN

        // Volatility (44-46)
        let candles = ctx.candles(symbol);
        let vol_8 = realized_volatility(candles, 8);
        let vol_32 = realized_volatility(candles, 32);
        features[extended_idx::REALIZED_VOL_8] = vol_8;
        features[extended_idx::REALIZED_VOL_32] = vol_32;
        features[extended_idx::VOL_RATIO] = if vol_32 > 0.0 && vol_32.is_finite() {
            vol_8 / vol_32
        } else { f64::NAN };

        // Fundamental (47-48)
        if let Some(fv) = ctx.fair_value(symbol) {
            let fv_f = fv.to_float();
            if fv_f > 0.0 {
                features[extended_idx::FAIR_VALUE_DEV] = (mid - fv_f) / fv_f;
                features[extended_idx::PRICE_TO_FAIR] = mid / fv_f;
            }
        }

        // Momentum quality (49-50)
        let ema_8 = ctx.get_indicator(symbol, IndicatorType::Ema(8)).unwrap_or(f64::NAN);
        let ema_16 = ctx.get_indicator(symbol, IndicatorType::Ema(16)).unwrap_or(f64::NAN);
        let atr_8 = ctx.get_indicator(symbol, IndicatorType::Atr(8)).unwrap_or(f64::NAN);
        features[extended_idx::TREND_STRENGTH] = if atr_8 > 0.0 && atr_8.is_finite() {
            (ema_8 - ema_16).abs() / atr_8
        } else { f64::NAN };

        let rsi_8 = ctx.get_indicator(symbol, IndicatorType::Rsi(8)).unwrap_or(f64::NAN);
        features[extended_idx::RSI_DIVERGENCE] = if rsi_8.is_finite() { rsi_8 - 50.0 } else { f64::NAN };

        // Volume dynamics (51-52)
        // Average EXCLUDES latest candle to avoid dampening the signal
        let n_prev = candles.len().saturating_sub(1).min(8);
        let vol_avg = if n_prev > 0 {
            candles.iter().rev().skip(1).take(8).map(|c| c.volume.raw() as f64).sum::<f64>() / n_prev as f64
        } else { 0.0 };
        let latest_vol = ctx.last_candle(symbol).map(|c| c.volume.raw() as f64).unwrap_or(f64::NAN);
        features[extended_idx::VOLUME_SURGE] = if vol_avg > 0.0 { latest_vol / vol_avg } else { f64::NAN };
        features[extended_idx::TRADE_INTENSITY] = trades.len() as f64 / 8.0;

        // Interaction term (53) — MUST be computed BEFORE NaN imputation
        let sentiment = ctx.symbol_sentiment(symbol);
        let fv_dev = features[extended_idx::FAIR_VALUE_DEV];
        features[extended_idx::SENTIMENT_PRICE_GAP] = if fv_dev.is_nan() {
            f64::NAN  // Missing fundamentals → missing interaction
        } else {
            sentiment * fv_dev
        };

        // Per-feature neutral value imputation (replaces uniform -1.0)
        features.iter_mut().enumerate().for_each(|(i, f)| {
            if f.is_nan() {
                *f = FULL_FEATURE_NEUTRAL[i];
            }
        });

        features
    }

    fn feature_names(&self) -> &[&str] { FULL_FEATURE_NAMES }
    fn neutral_values(&self) -> &[f64] { &FULL_FEATURE_NEUTRAL }
}
```

### Recording Update

**Prerequisite**: Pre-V6 refactor section F complete — features are pre-extracted in the runner (Phase 3) and passed to `RecordingHook` via `EnrichedData.features`. This eliminates the dual code path (`MarketFeatures::extract()` vs `extract_features()`) that guaranteed training-serving skew. Training-serving parity is now guaranteed by construction.

Update `--headless-record` to accept a `FeatureExtractor`:
- CLI flag: `--full-features` (default: MinimalFeatures for backward compatibility)
- When `--full-features`: uses `FullFeatures` extractor, Parquet has 55 columns
- Existing recordings with 42 columns remain valid

---

## Files Summary

| File | Action |
|------|--------|
| `crates/types/src/features.rs` | MODIFY — add extended constants, indices, names, pure functions, `FULL_FEATURE_NEUTRAL` |
| `crates/agents/src/tier1/ml/mod.rs` | MODIFY — add `neutral_values()` to `FeatureExtractor` trait |
| `crates/agents/src/tier1/ml/feature_extractor.rs` | MODIFY — add `neutral_values()` to `MinimalFeatures` |
| `crates/agents/src/tier1/ml/full_features.rs` | CREATE — `FullFeatures` implementing `FeatureExtractor` |
| `crates/agents/src/lib.rs` | MODIFY — export `FullFeatures` |
| `crates/simulation/src/hooks.rs` | MODIFY — add `recent_trades` to `EnrichedData` (section F) |
| `crates/storage/src/recording_hook.rs` | MODIFY — use `FeatureExtractor` via `StrategyContext` (section F) |
| `src/main.rs` | MODIFY — add `--full-features` CLI flag |

---

## Feature Validation Plan (post-implementation, pre-V6.2)

1. **Record 50k ticks** with `--headless-record --full-features --ticks 50000`
2. **NaN rate report**: For each feature, compute fraction of ticks where the raw value was NaN (before imputation). Features with >80% NaN carry no signal — they are effectively constants (the neutral value). Pay special attention to `spread_bps` and `book_imbalance` which may be frequently NaN in batch auction markets.
3. **Correlation matrix**: In Python, compute pairwise Pearson correlation. Flag features with |r| > 0.95 as redundant.
3. **Mutual information**: Compute MI between each feature and next-tick return sign. Features with MI < 0.01 are likely noise.
4. **Compare `f_book_imbalance` vs `f_net_order_flow`**: Compute MI for both. If correlated (|r| > 0.8), keep whichever has higher MI.
5. **Feature importance (post-V6.2)**: After ensemble training, run SHAP analysis. Features with near-zero SHAP values across all models are candidates for removal.
6. **Ablation (post-V6.2)**: Retrain ensemble with one feature group disabled at a time. Measure accuracy drop to quantify each group's contribution.
7. **Trimming (post-V6.2)**: Remove features identified in steps 2-6. Update `N_FULL_FEATURES`, re-index, retrain.

---

## Verification

1. `cargo test --workspace` — all tests pass, new pure functions have unit tests
2. `cargo run -- --headless-record --full-features --ticks 500` — produces Parquet with 55 columns
3. Python: `pd.read_parquet("data/training_000.parquet")` — all 55 columns present, no NaN in imputed columns
4. **Imputation check**: Verify NaN values are imputed to per-feature neutrals (not uniform -1.0). Check: `f_vol_ratio` missing → 1.0, `f_rsi_divergence` missing → 0.0, `f_bb_percent_b` missing → 0.5
5. Backward compat: `cargo run -- --headless-record --ticks 500` (without `--full-features`) — still produces 42-column Parquet with -1.0 imputation
6. Feature ranges: spot-check that spread_bps in [0, 1000], book_imbalance in [-1, 1], net_order_flow in [-1, 1], vol in [0, 1], etc.
7. **Realized vol sanity**: Verify `realized_vol_8` < `realized_vol_32` is NOT always true (it shouldn't be — vol can expand or contract)
8. **Recording path parity**: Compare inference features (from ML cache) with recorded features for same tick — they should be identical (section F validation)
9. **Feature audit**: Run MI analysis and correlation check on 50k tick recording.
