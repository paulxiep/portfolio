# V6.2: Full Ensemble

## Overview

Add `LinearModel` and `SvmLinear` implementing `MlModel`, then combine all model types into an `EnsembleAgent` using weighted voting. The ensemble's performance becomes the baseline that V7.1 RL must beat.

**Requires**: V6.1 complete (`FullFeatures` extractor, extended features in ML cache)

## Design Decisions

### 1. LinearModel and SvmLinear Are Structurally Identical

Both compute `softmax(dot(weights, features) + bias)`. The difference is in how they're *trained* (logistic loss vs hinge loss), not in Rust inference. Still implemented as separate structs for:
- Clear JSON `model_type` field for model selection
- Potential future divergence (SVM with RBF kernel would differ)
- Descriptive type names in logs

### 2. EnsembleAgent Is NOT TreeAgent

`TreeAgent<M>` uses the centralized prediction cache — it looks up pre-computed `[p_sell, p_hold, p_buy]` per (model, symbol). This works because each agent uses one model.

`EnsembleAgent` combines *multiple* models with weighted voting. It:
1. Gets cached features (not predictions) from `MlPredictionCache`
2. Runs each model locally: `model.predict(&features[..model.n_features()])`
3. Averages probabilities with per-model weights
4. Applies threshold logic (same as TreeAgent)

This is a new `Agent` impl, not generic over `MlModel`.

### 3. Mixed Feature Counts

The ensemble can combine V5 models (trained on 42 features) with V6 models (trained on 54 features). Each model receives `features[..model.n_features()]`. This works because V5 features are a prefix of V6 features — the first 42 are identical.

### 4. Model Weights

Weights can be:
- **Uniform**: `[1.0, 1.0, 1.0]` (equal voting)
- **Accuracy-based**: Set proportional to validation accuracy during Python training
- **Learned**: V7.1 can learn optimal weights via reward optimization

Default: uniform. Weights specified in the ensemble JSON config.

---

## New Model Types

### LinearModel (`linear_model.rs`)

```rust
use serde::Deserialize;
use super::{ClassProbabilities, MlModel, softmax};

#[derive(Debug, Deserialize)]
struct LinearModelJson {
    model_type: String,     // "linear_model"
    model_name: String,
    n_features: usize,
    n_classes: usize,       // 3
    classes: Vec<i32>,      // [-1, 0, 1]
    coefficients: Vec<Vec<f64>>,  // [n_classes][n_features]
    intercepts: Vec<f64>,         // [n_classes]
}

pub struct LinearModel {
    name: String,
    coefficients: Vec<Vec<f64>>,
    intercepts: Vec<f64>,
    n_features: usize,
}

impl LinearModel {
    pub fn from_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {
        let json_str = std::fs::read_to_string(path.as_ref())
            .map_err(|e| format!("Failed to read {}: {}", path.as_ref().display(), e))?;
        Self::from_json_str(&json_str)
    }

    pub fn from_json_str(json: &str) -> Result<Self, String> {
        let parsed: LinearModelJson = serde_json::from_str(json)
            .map_err(|e| format!("JSON parse error: {}", e))?;

        // Validation
        if parsed.n_classes != 3 { return Err("Expected 3 classes".into()); }
        if parsed.coefficients.len() != 3 { return Err("Expected 3 coefficient rows".into()); }
        for (i, row) in parsed.coefficients.iter().enumerate() {
            if row.len() != parsed.n_features {
                return Err(format!("Row {} has {} coefficients, expected {}", i, row.len(), parsed.n_features));
            }
        }
        if parsed.intercepts.len() != 3 { return Err("Expected 3 intercepts".into()); }

        let name = format!("LinearModel_{}", parsed.model_name);
        Ok(Self { name, coefficients: parsed.coefficients, intercepts: parsed.intercepts, n_features: parsed.n_features })
    }
}

impl MlModel for LinearModel {
    fn predict(&self, features: &[f64]) -> ClassProbabilities {
        let mut scores = [0.0f64; 3];
        for class in 0..3 {
            scores[class] = self.coefficients[class].iter()
                .zip(features.iter())
                .map(|(c, f)| c * f)
                .sum::<f64>() + self.intercepts[class];
        }
        softmax(&scores)
    }

    fn name(&self) -> &str { &self.name }
    fn n_features(&self) -> usize { self.n_features }
}
```

### SvmLinear (`svm_linear.rs`)

Structurally identical to LinearModel. JSON has `weights` and `biases` instead of `coefficients` and `intercepts`:

```rust
#[derive(Debug, Deserialize)]
struct SvmLinearJson {
    model_type: String,     // "svm_linear"
    model_name: String,
    n_features: usize,
    n_classes: usize,
    classes: Vec<i32>,
    weights: Vec<Vec<f64>>,   // [n_classes][n_features]
    biases: Vec<f64>,         // [n_classes]
}

pub struct SvmLinear {
    name: String,
    weights: Vec<Vec<f64>>,
    biases: Vec<f64>,
    n_features: usize,
}

impl MlModel for SvmLinear {
    fn predict(&self, features: &[f64]) -> ClassProbabilities {
        let mut scores = [0.0f64; 3];
        for class in 0..3 {
            scores[class] = self.weights[class].iter()
                .zip(features.iter())
                .map(|(w, f)| w * f)
                .sum::<f64>() + self.biases[class];
        }
        softmax(&scores)
    }

    fn name(&self) -> &str { &self.name }
    fn n_features(&self) -> usize { self.n_features }
}
```

---

## EnsembleAgent

```rust
use std::sync::Arc;

pub struct EnsembleConfig {
    pub symbols: Vec<Symbol>,
    pub buy_threshold: f64,
    pub sell_threshold: f64,
    pub order_size: u64,
    pub max_long_position: i64,
    pub max_short_position: i64,
    pub initial_cash: Cash,
    pub initial_price: Price,
}

pub struct EnsembleAgent {
    id: AgentId,
    config: EnsembleConfig,
    state: AgentState,
    models: Vec<Arc<dyn MlModel>>,
    weights: Vec<f64>,
}

impl EnsembleAgent {
    pub fn new(
        id: AgentId,
        models: Vec<Arc<dyn MlModel>>,
        weights: Vec<f64>,
        config: EnsembleConfig,
    ) -> Self {
        assert_eq!(models.len(), weights.len(), "models and weights must match");
        let state = AgentState::with_symbols(config.initial_cash, config.symbols.clone());
        Self { id, config, state, models, weights }
    }

    fn weighted_vote(&self, features: &[f64]) -> ClassProbabilities {
        let mut weighted = [0.0f64; 3];
        let mut total_weight = 0.0;

        for (model, &weight) in self.models.iter().zip(&self.weights) {
            // Each model uses features up to its expected count
            // V5 models (42 features): features[..42]
            // V6 models (54 features): features[..54]
            let n = model.n_features().min(features.len());
            let probs = model.predict(&features[..n]);
            for i in 0..3 {
                weighted[i] += probs[i] * weight;
            }
            total_weight += weight;
        }

        if total_weight > 0.0 {
            for p in &mut weighted { *p /= total_weight; }
        }
        weighted
    }
}

impl Agent for EnsembleAgent {
    fn id(&self) -> AgentId { self.id }

    fn on_tick(&mut self, ctx: &StrategyContext<'_>) -> AgentAction {
        let mut orders = Vec::new();

        for symbol in &self.config.symbols {
            // Get cached features (54 elements from FullFeatures extractor)
            let features = match ctx.ml_cache()
                .and_then(|cache| cache.get_features(symbol))
            {
                Some(f) => f,
                None => continue,
            };

            // Weighted ensemble prediction
            let probs = self.weighted_vote(features);
            let p_sell = probs[0];
            let p_hold = probs[1];
            let p_buy = probs[2];

            let mid_price = ctx.mid_price(symbol)
                .or(ctx.last_price(symbol))
                .unwrap_or(self.config.initial_price);

            // Same threshold logic as TreeAgent
            let buy = p_buy > self.config.buy_threshold && p_buy >= p_hold
                && self.state.position_for(symbol) < self.config.max_long_position;
            let sell = p_sell > self.config.sell_threshold && p_sell >= p_hold
                && self.state.position_for(symbol) > -self.config.max_short_position;

            match (sell, buy) {
                (true, true) if p_buy >= p_sell => {
                    orders.push(generate_buy_order(self.id, symbol, mid_price, self.config.order_size));
                }
                (true, _) => {
                    orders.push(generate_sell_order(self.id, symbol, mid_price, self.config.order_size));
                }
                (_, true) => {
                    orders.push(generate_buy_order(self.id, symbol, mid_price, self.config.order_size));
                }
                _ => {}
            }
        }

        if orders.is_empty() { AgentAction::none() }
        else { AgentAction::multiple(orders) }
    }

    fn on_fill(&mut self, trade: &Trade) {
        if trade.buyer_id == self.id {
            self.state.on_buy(&trade.symbol, trade.quantity.raw(), trade.value());
        }
        if trade.seller_id == self.id {
            self.state.on_sell(&trade.symbol, trade.quantity.raw(), trade.value());
        }
    }

    fn name(&self) -> &str { "EnsembleAgent" }
    fn is_ml_agent(&self) -> bool { true }
    fn state(&self) -> &AgentState { &self.state }
}
```

---

## Python Training Scripts

### `scripts/train_ensemble.py`

```python
"""Train ensemble models (RandomForest + LogisticRegression + LinearSVC) on recorded features."""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import json, sys

# Load recorded features
df = pd.read_parquet(sys.argv[1] if len(sys.argv) > 1 else "data/training_000.parquet")
feature_cols = [c for c in df.columns if c.startswith("f_")]
X = df[feature_cols].values
# Label: next-tick price direction
y = np.sign(df["f_mid_price"].shift(-1) - df["f_mid_price"]).fillna(0).astype(int).values
X, y = X[:-1], y[:-1]  # Drop last row (no next-tick label)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X = np.nan_to_num(X, nan=-1.0)
X_train = np.nan_to_num(X_train, nan=-1.0)
X_test = np.nan_to_num(X_test, nan=-1.0)

# Train models
rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
rf.fit(X_train, y_train)
print("RandomForest:", classification_report(y_test, rf.predict(X_test)))

lr = LogisticRegression(multi_class="multinomial", max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
print("LogisticRegression:", classification_report(y_test, lr.predict(X_test)))

svm = LinearSVC(multi_class="crammer_singer", max_iter=2000, random_state=42)
svm.fit(X_train, y_train)
print("LinearSVC:", classification_report(y_test, svm.predict(X_test)))
```

### `scripts/export_linear.py`

```python
"""Export trained LogisticRegression to JSON for Rust inference."""
import json, sys

# Assumes model is already trained (import from train_ensemble.py or load pickle)
export = {
    "model_type": "linear_model",
    "model_name": "logistic_regression_v6",
    "n_features": lr.coef_.shape[1],
    "n_classes": 3,
    "classes": [-1, 0, 1],
    "coefficients": lr.coef_.tolist(),
    "intercepts": lr.intercept_.tolist(),
}
with open("models/logistic_regression.json", "w") as f:
    json.dump(export, f, indent=2)
print(f"Exported: {lr.coef_.shape[1]} features, {lr.coef_.shape[0]} classes")
```

### `scripts/export_svm.py`

```python
"""Export trained LinearSVC to JSON for Rust inference."""
import json

export = {
    "model_type": "svm_linear",
    "model_name": "linear_svc_v6",
    "n_features": svm.coef_.shape[1],
    "n_classes": 3,
    "classes": [-1, 0, 1],
    "weights": svm.coef_.tolist(),
    "biases": svm.intercept_.tolist(),
}
with open("models/linear_svc.json", "w") as f:
    json.dump(export, f, indent=2)
print(f"Exported: {svm.coef_.shape[1]} features, {svm.coef_.shape[0]} classes")
```

---

## Files Summary

| File | Action |
|------|--------|
| `crates/agents/src/tier1/ml/linear_model.rs` | CREATE |
| `crates/agents/src/tier1/ml/svm_linear.rs` | CREATE |
| `crates/agents/src/tier1/ml/ensemble_agent.rs` | CREATE |
| `crates/agents/src/tier1/ml/mod.rs` | MODIFY — add `pub mod` and `pub use` for new types |
| `crates/agents/src/lib.rs` | MODIFY — export new types |
| `src/main.rs` | MODIFY — add ensemble agent spawning to `AgentFactory` |
| `scripts/train_ensemble.py` | CREATE |
| `scripts/export_linear.py` | CREATE |
| `scripts/export_svm.py` | CREATE |

---

## Verification

1. **Unit tests**: Load each model from JSON, verify predictions sum to 1.0, verify correct class ordering
2. **Ensemble test**: Create ensemble with mock models of known outputs, verify weighted voting
3. **Integration test**: Run simulation with ensemble agents for 5000 ticks, verify they trade and have non-zero P&L
4. **Benchmark**: Measure ensemble prediction latency
   - Target: <50us per ensemble prediction (3 models x 54 features)
   - LinearModel/SvmLinear: target ~200ns (dot product)
   - RandomForest: target ~20us (50 trees)
   - Total overhead: ~21us
5. **Python round-trip**: Train model in Python → export JSON → load in Rust → verify identical predictions on test vectors
6. **SHAP analysis**: After training, run `scripts/analyze_shap.py` on ensemble to identify feature importance
